{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "basic_dense_GAN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPtw8OfCSXtXrPEYSn4DdBn"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "f = open('dataset1505.json')\n",
        "\n",
        "dataset = json.load(f)"
      ],
      "metadata": {
        "id": "B7wHDqncOpI_"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Asa0Jj5LPR9T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b89b878-d489-435e-a128-6a394db9c9af"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = np.array(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GUbVr3lSEN0",
        "outputId": "379f7006-b5a0-4428-977c-f8aea57f8342"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(dataset[0,0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRwW4rXFTYna",
        "outputId": "9012304c-8b10-4e0e-e0c8-cd2736f67cb2"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "245\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ctSjZBUdCve0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_y(x):\n",
        "    return 10 + x*x\n",
        "\n",
        "\n",
        "def sample_data(n=10000, scale=100):\n",
        "    data = []\n",
        "\n",
        "    x = scale*(np.random.random_sample((n,))-0.5)\n",
        "\n",
        "    for i in range(n):\n",
        "        yi = get_y(x[i])\n",
        "        data.append([x[i], yi])\n",
        "\n",
        "    return np.array(data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generator(Z,hsize=[16, 16],reuse=False):\n",
        "    with tf.variable_scope(\"GAN/Generator\",reuse=reuse):\n",
        "        h1 = tf.layers.dense(Z,hsize[0],activation=tf.nn.leaky_relu)\n",
        "        h2 = tf.layers.dense(h1,hsize[1],activation=tf.nn.leaky_relu)\n",
        "        out = tf.layers.dense(h2,2)\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "XqsHgkztD0mY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def discriminator(X,hsize=[16, 16],reuse=False):\n",
        "    with tf.variable_scope(\"GAN/Discriminator\",reuse=reuse):\n",
        "        h1 = tf.layers.dense(X,hsize[0],activation=tf.nn.leaky_relu)\n",
        "        h2 = tf.layers.dense(h1,hsize[1],activation=tf.nn.leaky_relu)\n",
        "        h3 = tf.layers.dense(h2,2)\n",
        "        out = tf.layers.dense(h3,1)\n",
        "\n",
        "    return out, h3"
      ],
      "metadata": {
        "id": "djl1Jx4LEPSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = tf.placeholder(tf.float32,[None,2])\n",
        "Z = tf.placeholder(tf.float32,[None,2])"
      ],
      "metadata": {
        "id": "K1Vvhy95FDij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "G_sample = generator(Z)\n",
        "r_logits, r_rep = discriminator(X)\n",
        "f_logits, g_rep = discriminator(G_sample,reuse=True)"
      ],
      "metadata": {
        "id": "lkIhjQNAFfgO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "disc_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=r_logits,labels=tf.ones_like(r_logits)) + tf.nn.sigmoid_cross_entropy_with_logits(logits=f_logits,labels=tf.zeros_like(f_logits)))\n",
        "gen_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=f_logits,labels=tf.ones_like(f_logits)))\n"
      ],
      "metadata": {
        "id": "R_jIernvFpR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,scope=\"GAN/Generator\")\n",
        "disc_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,scope=\"GAN/Discriminator\")\n",
        "\n",
        "gen_step = tf.train.RMSPropOptimizer(learning_rate=0.001).minimize(gen_loss,var_list = gen_vars) # G Train step\n",
        "disc_step = tf.train.RMSPropOptimizer(learning_rate=0.001).minimize(disc_loss,var_list = disc_vars) # D Train step\n"
      ],
      "metadata": {
        "id": "weLlqs7RGVgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(100001):\n",
        "    X_batch = sample_data(n=batch_size)\n",
        "    Z_batch = sample_Z(batch_size, 2)\n",
        "    _, dloss = sess.run([disc_step, disc_loss], feed_dict={X: X_batch, Z: Z_batch})\n",
        "    _, gloss = sess.run([gen_step, gen_loss], feed_dict={Z: Z_batch})\n",
        "\n",
        "    print \"Iterations: %d\\t Discriminator loss: %.4f\\t Generator loss: %.4f\"%(i,dloss,gloss)\n",
        "  "
      ],
      "metadata": {
        "id": "iftvHnSrGoG7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}